- time: 9:45 - 10:00
  info: "<b>Day 2 Overview</b> <i>Lecture Hall 3F</i><br>Information for the day"
- time: 10:00 - 11:00
  info: "<b>Keynote talk by Dr. Nurul Lubis</b> <i>Lecture Hall 3F</i><br><details><summary>▼ Dialogue Evaluation via Offline Reinforcement Learning and Emotion Prediction</summary> [SLIDES]<br>Task-oriented dialogue systems aim to fulfill user goals, such as booking hotels or searching for restaurants, through natural language interactions. They are ideally evaluated through interaction with human users. However, this is unattainable to do at every iteration of the development phase due to time and financial constraints. Therefore, researchers resort to static evaluation on dialogue corpora. Although they are more practical and easily reproducible, they do not fully reflect real performance of dialogue systems. Can we devise an evaluation that keeps the best of both worlds? In this talk I explore the usage of offline reinforcement learning and emotion prediction for dialogue evaluation that is practical, reliable, and strongly correlated with human judgements.</details>"
- time: 11:00-11:30
  info: "<b>Coffee Break</b> <i>23.21.00.44</i><br>Networking"
- time: 11:30-12:00
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.046</i><br><details><summary>▼ Exploring Song Topics Across Different Countries - A Latent Dirichlet Allocation Approach</summary> [SLIDES]<br>[ABSTRACT TEXT]</details><hr><b>Student talk</b> <i>Seminar Room 23.21.00.048</i><br><details><summary>▼ Microsyntactic Unit Analysis using Word Embedding Models - Experiments on Slavic Languages</summary> [SLIDES]<br>Microsyntactic units have been defined as language-specific transitional entities between lexicon and grammar, which idiomatic properties are closely tied to syntax. While these units are abundant and diverse, they are typically described based on individual constructions,making their comprehensive understanding difficult. This study proposes a novel approach to detect microsyntacticunits using Word Embedding Models (WEMs) trained on six Slavic languages, namely, Belarusian, Ukrainian, Russian, Bulgarian, Czech, and Polish and evaluates how well these models capture the nuances of syntactic compositionality.To address this challenge, we apply two different WEMs that previously proved effective at idiomaticity detection, namely Word2Vec CBOW and Context2Vec, as well as three adaptations of Word2Vec for syntactic tasks, namely Word2Vec CWINDOW, Word2Vec Structured, and Node2Vec. The training data is sourced primarily from the Leipzig Corpora Collection and the Russian National Corpus. To evaluate the models, we develop a cross-lingual inventory of microsyntactic units using the lists of microsyntantic units available at the Russian National Corpus. We extracted 50 most frequent microsyntactic units from each category (prepositions, adverbial and predicatives, parenthetical expressions, conjunctions, and particles), resulting in parallel sets of 227 microsyntactic units with their context sentences, each for one of the six Slavic languages under analysis. Our results demonstrate the effectiveness of WEMs in capturing microsyntactic units and identifying their compositionality. We find that simple Word2Vec embedding models adapted for syntactic tasks perform best, even when compared to neural-based DSMs. We show that the behavior of WEMs is consistent across all six Slavic languages under analysis, validating our proposed approach as applicable and effective for identifying microsyntactic units. Our findings contribute to the theory of microsyntax by providing insights into the detection of microsyntactic units and their crosslinguistic properties. Our approach has practical applications in natural language processing, machine translation, and computational linguistics, where the identification of microsyntactic units can improve the accuracy of tasks such as syntactic parsing and named entity recognition. </details>"  
- time: 12:10-12:40
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.046</i><br><details><summary>▼ Evaluation of Russian Noun Word Embeddings For Cases and a Number</summary> [SLIDES]<br>Russian can be characterized by a rich inflectional morphology. Particularly Russian nouns can illustrate this variety by changing the word form to indicate its grammatical case and number. The word embeddings for each of the case noun forms in singular and plural will be represented by its own real-valued vector of the length of 300 dimensions that encodes word meaning, so that words that are close in the embedding space should also be close in their meaning. Some other languages that have different noun forms for a number and/or case may exhibit interesting features in word embeddings. For example, Shafaei-Bajestan et al. (2022) have examined the semantic properties of English nominal pluralization and word embeddings and have found out that shift vectors for words, that belong to different groups on the basis of their meaning, are substantially different. This research has encouraged us to look closer at Russian noun embeddings and figure out if nouns that belong to the same group show any regularities on the basis of case forms and number. In order to do that, the dataset of 1700 nouns has been divided into groups on the basis of their semantical similarity, then enlarged with 12 columns that correspond to six Russian noun cases in two numbers (singular and plural). After that the fastText library is applied to get word vectors for each of the noun case forms. Then the difference vectors between base form (nominative case) and the other cases are calculated for each noun in the dataset. The average vectors are calculated for each group of words for every case form. The resulting vectors are added to be the base word form and then compared to the initial fastText vector. We assume that words that belong to the same group may have the same average vector for every case form. The results will help us to understand word embeddings better, thus improve word representations</details><hr><b>Student talk</b>  <i>Seminar Room 23.21.00.048</i><br><details><summary>▼ How do You measures Style (And Much More)</summary> [SLIDES]<br>If you were to give ten English scholars two English texts and ask which one was written by Jane Austen and which – by Charlotte Brontë, they would most probably have no difficulty in answering correctly. However, if you were to ask them to explain the motivation behind their answer, their responses would most definitely differ from each other. This little thought experiment begs the question - how do you automate that task? What exactly do you give to a computer to make it understand that two texts are written by different authors? Moreover, how do you make your algorithm not depend on a particular language? This is the problem of automatic authorship attribution. To resolve this, many scholars have tried to involve statistical methods. Only one rather simple method, however, seemed to stick – Burrow’s Delta. Invented by John Burrows in 2002, the Delta Analysis has proved in time to be a robust instrument for authorship attribution. To identify, whether a document was written by Author A or Author B, you would need to collect a corpus of written texts and see, which author’s “style” the document is most similar to. Of course, the scope of stylometry goes beyond authorship - many methods have been derived to compare different stylistic qualities of two authors. In that context, we will discuss one instrument in particular - the Zeta Analysis, which compares two corpora by extracting their keywords. In this talk, we will look “under the hood” of the two methods and try to understand how exactly they succeed in their tasks, what their advantages and drawbacks are. Come join and learn about how Digital Humanities deals with the intricacies of</details>"
- time: 12:40 - 13:30
  info: "<b>Lunch</b> <i>University Mensa</i>"
- time: 13:30-15:30
  info: "<b>Career Networking Meet-up</b> <i>Seminar Room 23.21.00.044 </i><br>Opportunity to discover future career paths and learn from people in the industry and academia. Sponsors from industry and academics from HHU will be there to tell you about their careers in computational linguistics and answers questions about yours." 
- time: 15:30 - 16:30
  info: "<b>Keynote talk by Apl.Prof Wiebke Petersen</b> <i>Lecture Hall 3F</i><br>TBA"
- time: 16:30 - Open
  info: "<b>Boardgames</b><br><br>Socialising time"
- time: 20:00 - Open
  info: "<b>Social time in the Altstadt</b><br><br>Socialising time"